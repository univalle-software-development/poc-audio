# POC Audio - ImplementaciÃ³n de Google Cloud Speech-to-Text

## Universidad del Valle - Proyecto Integrador II-01
**Equipo**: Golangers  
**Fecha**: Octubre 2025

---

## DescripciÃ³n

Este proyecto es una prueba de concepto (POC) que implementa Google Cloud Speech-to-Text API en una aplicaciÃ³n web de chat construida con Next.js. El objetivo es demostrar cÃ³mo capturar audio del navegador, enviarlo a Google Cloud para transcripciÃ³n y utilizar el texto resultante en una conversaciÃ³n con IA.

---

## Arquitectura de Speech-to-Text

La implementaciÃ³n se divide en tres componentes principales:

1. **Cliente (Navegador)**: Captura de audio usando MediaRecorder API
2. **Servidor (Next.js API Route)**: Proxy seguro hacia Google Cloud
3. **Google Cloud Speech-to-Text**: Servicio de transcripciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Navegador     â”‚â”€â”€â”€â”€â”€â–¶â”‚   Next.js API    â”‚â”€â”€â”€â”€â”€â–¶â”‚  Google Cloud STT   â”‚
â”‚  MediaRecorder  â”‚      â”‚   /api/speech    â”‚      â”‚   (TranscripciÃ³n)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                          â”‚
         â”‚ Audio Blob             â”‚ Base64                   â”‚ JSON
         â”‚ (WebM Opus)            â”‚ Audio                    â”‚ {text}
         â–¼                        â–¼                          â–¼
```

---

## 1. Captura de Audio en el Cliente

### Archivo: `hooks/use-speech-to-text.ts`

Este hook personalizado de React gestiona todo el proceso de grabaciÃ³n y transcripciÃ³n de audio.

### CÃ³digo Explicado

```typescript
export function useSpeechToText() {
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [transcript, setTranscript] = useState<string>("");
  const [error, setError] = useState<string | null>(null);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
```

**Â¿Por quÃ© usar refs?**
- `mediaRecorderRef`: Necesitamos mantener la referencia al objeto MediaRecorder entre renders sin causar re-renderizados innecesarios
- `audioChunksRef`: Los chunks de audio se acumulan durante la grabaciÃ³n. Usar un ref evita que React re-renderice cada vez que se agrega un chunk

### Inicio de GrabaciÃ³n

```typescript
const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    
    const mediaRecorder = new MediaRecorder(stream, {
      mimeType: "audio/webm;codecs=opus",
    });
```

**Â¿Por quÃ© WebM Opus?**
1. **Compatibilidad**: Es el formato predeterminado en Chrome y Firefox
2. **Eficiencia**: Opus es un cÃ³dec de alta compresiÃ³n diseÃ±ado para voz
3. **Calidad**: Mantiene buena calidad con tamaÃ±os de archivo pequeÃ±os
4. **Soporte nativo**: Google Cloud Speech-to-Text soporta WebM Opus directamente

```typescript
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunksRef.current.push(event.data);
      }
    };
```

**Â¿Por quÃ© acumular chunks?**
- MediaRecorder genera datos en fragmentos durante la grabaciÃ³n
- Acumularlos permite construir el audio completo al final
- Si intentÃ¡ramos procesar cada chunk individualmente, tendrÃ­amos transcripciones parciales y fragmentadas

### DetenciÃ³n de GrabaciÃ³n y TranscripciÃ³n

```typescript
const stopRecording = async () => {
  return new Promise<void>((resolve) => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, {
          type: "audio/webm;codecs=opus",
        });
```

**Â¿Por quÃ© crear un Blob?**
- Los chunks individuales no son un archivo de audio vÃ¡lido
- `new Blob()` combina todos los chunks en un archivo coherente
- El tipo MIME especifica el formato para que el servidor lo interprete correctamente

```typescript
        const formData = new FormData();
        formData.append("audio", audioBlob, "recording.webm");

        setIsTranscribing(true);

        try {
          const response = await fetch("/api/speech-to-text", {
            method: "POST",
            body: formData,
          });
```

**Â¿Por quÃ© usar FormData?**
1. **EstÃ¡ndar web**: FormData es el mÃ©todo estÃ¡ndar para enviar archivos en HTTP
2. **Compatibilidad**: Next.js API Routes lo entienden nativamente
3. **Multipart**: Permite enviar archivos binarios (audio) junto con otros datos si fuera necesario

**Â¿Por quÃ© no enviar base64 directamente desde el cliente?**
- SerÃ­a mÃ¡s eficiente enviar el Blob directamente al servidor
- El servidor se encarga de la conversiÃ³n a base64 solo cuando es necesario (para Google Cloud)
- Esto reduce el procesamiento en el cliente y el tamaÃ±o de la transferencia

---

## 2. Endpoint del Servidor

### Archivo: `app/api/speech-to-text/route.ts`

Este endpoint actÃºa como proxy entre el cliente y Google Cloud.

### CÃ³digo Completo Explicado

```typescript
import { NextRequest, NextResponse } from "next/server";

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData();
    const audioFile = formData.get("audio") as Blob;
```

**Â¿Por quÃ© validar que el audio existe?**
```typescript
    if (!audioFile) {
      return NextResponse.json(
        { error: "No audio file provided" },
        { status: 400 }
      );
    }
```
- Previene errores en pasos posteriores
- Retorna un error HTTP 400 (Bad Request) claro al cliente
- Mejora la experiencia de debugging

### ConversiÃ³n a Base64

```typescript
    const arrayBuffer = await audioFile.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);
    const base64Audio = buffer.toString("base64");
```

**Â¿Por quÃ© esta cadena de conversiones?**

1. **`audioFile.arrayBuffer()`**: Convierte el Blob en un ArrayBuffer (datos binarios en JavaScript)
2. **`Buffer.from(arrayBuffer)`**: Node.js usa `Buffer` para manipular datos binarios (ArrayBuffer es de navegador)
3. **`buffer.toString("base64")`**: Google Cloud API requiere el audio en base64 para JSON

**Â¿Por quÃ© Google Cloud requiere base64?**
- REST APIs solo transportan texto (JSON)
- Base64 es el estÃ¡ndar para codificar datos binarios como texto
- Permite enviar audio en un objeto JSON sin corromper los datos

### ConfiguraciÃ³n para Google Cloud

```typescript
    const apiKey = process.env.GOOGLE_CLOUD_API_KEY;
    
    if (!apiKey) {
      console.error("GOOGLE_CLOUD_API_KEY is not set");
      return NextResponse.json(
        { error: "Server configuration error" },
        { status: 500 }
      );
    }
```

**Â¿Por quÃ© verificar la API key en cada request?**
- Evita que la aplicaciÃ³n falle silenciosamente
- Proporciona feedback claro durante el desarrollo
- Retorna 500 (Server Error) porque es un problema de configuraciÃ³n, no del cliente

### ConstrucciÃ³n de la Solicitud

```typescript
    const response = await fetch(
      `https://speech.googleapis.com/v1/speech:recognize?key=${apiKey}`,
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          config: {
            encoding: "WEBM_OPUS",
            sampleRateHertz: 48000,
            languageCode: "es-ES",
            alternativeLanguageCodes: ["en-US"],
          },
          audio: {
            content: base64Audio,
          },
        }),
      }
    );
```

**Desglose de la configuraciÃ³n:**

#### `encoding: "WEBM_OPUS"`
- Debe coincidir exactamente con el formato que enviÃ³ el cliente
- Si no coincide, Google Cloud no podrÃ¡ decodificar el audio
- Otros valores posibles: `LINEAR16`, `FLAC`, `MP3`, etc.

#### `sampleRateHertz: 48000`
- Frecuencia de muestreo del audio
- 48kHz es el estÃ¡ndar para grabaciones de alta calidad
- Debe coincidir con la tasa de muestreo real del audio
- Valores comunes: 16000 (telÃ©fono), 44100 (CD), 48000 (audio profesional)

**Â¿Por quÃ© es importante la tasa de muestreo?**
- Una tasa incorrecta harÃ¡ que Google Cloud interprete mal la frecuencia de la voz
- ResultarÃ¡ en transcripciones incorrectas o errores

#### `languageCode: "es-ES"`
- Idioma principal para el reconocimiento de voz
- "es-ES" = EspaÃ±ol de EspaÃ±a
- Otras opciones: "es-MX" (MÃ©xico), "es-CO" (Colombia), etc.

**Â¿Importa la variante regional?**
- SÃ­, cada variante tiene modelos entrenados para acentos y vocabulario regional
- Mejora la precisiÃ³n para palabras y expresiones locales

#### `alternativeLanguageCodes: ["en-US"]`
- Idiomas de respaldo si detecta que no es espaÃ±ol
- Ãštil en aplicaciones multilingÃ¼es
- Google Cloud intenta primero "es-ES", si falla prueba "en-US"

### Procesamiento de la Respuesta

```typescript
    const data = await response.json();

    if (!response.ok) {
      console.error("Google Cloud API error:", data);
      return NextResponse.json(
        { error: "Transcription failed", details: data },
        { status: response.status }
      );
    }
```

**Â¿Por quÃ© verificar `response.ok`?**
- Google Cloud puede retornar 200 con un error en el body
- O retornar 4xx/5xx directamente
- Esta verificaciÃ³n cubre ambos casos

```typescript
    const transcript = data.results
      ?.map((result: any) => result.alternatives[0].transcript)
      .join(" ")
      .trim();
```

**Â¿QuÃ© significa esta estructura?**

Google Cloud retorna:
```json
{
  "results": [
    {
      "alternatives": [
        {
          "transcript": "hola mundo",
          "confidence": 0.95
        }
      ]
    },
    {
      "alternatives": [
        {
          "transcript": "cÃ³mo estÃ¡s",
          "confidence": 0.92
        }
      ]
    }
  ]
}
```

**Â¿Por quÃ© mÃºltiples `results`?**
- Google Cloud puede dividir el audio en segmentos
- Cada segmento tiene su propio resultado
- Necesitamos combinarlos para obtener la transcripciÃ³n completa

**Â¿Por quÃ© `alternatives[0]`?**
- Google Cloud puede retornar mÃºltiples alternativas de transcripciÃ³n
- La primera (`[0]`) es siempre la de mayor confianza
- Las demÃ¡s son alternativas menos probables

```typescript
    if (!transcript) {
      return NextResponse.json(
        { error: "No transcription found" },
        { status: 400 }
      );
    }

    return NextResponse.json({ transcript });
```

**Â¿Por quÃ© validar que hay transcripciÃ³n?**
- El audio puede ser silencio o ruido
- Google Cloud retornarÃ­a `results: []`
- Mejor informar al usuario que no se detectÃ³ voz

---

## 3. ConfiguraciÃ³n de Google Cloud

### Pasos Previos Requeridos

#### 1. Crear un Proyecto en Google Cloud

```bash
# En Google Cloud Console
1. Ir a: https://console.cloud.google.com/
2. Crear nuevo proyecto: "poc-audio-univalle"
3. Anotar el Project ID
```

**Â¿Por quÃ© crear un proyecto dedicado?**
- AÃ­sla facturaciÃ³n y cuotas de otros proyectos
- Facilita administraciÃ³n de permisos
- Permite eliminar todos los recursos fÃ¡cilmente al terminar

#### 2. Habilitar la API de Speech-to-Text

```bash
# En la Cloud Console
1. Ir a "APIs & Services" > "Library"
2. Buscar "Cloud Speech-to-Text API"
3. Click en "Enable"
```

**Â¿Por quÃ© hay que habilitar la API?**
- Google Cloud tiene cientos de APIs
- Por seguridad y costos, todas estÃ¡n deshabilitadas por defecto
- Solo pagas por las que uses

#### 3. Crear una API Key

```bash
# En la Cloud Console
1. Ir a "APIs & Services" > "Credentials"
2. Click "Create Credentials" > "API Key"
3. Copiar la key generada
```

**Â¿Por quÃ© usar API Key y no OAuth?**
- **API Key**: AutenticaciÃ³n simple, ideal para server-to-server
- **OAuth**: Para apps que actÃºan en nombre de usuarios
- AquÃ­ el servidor llama a Google Cloud directamente, no necesitamos OAuth

#### 4. Restringir la API Key (Importante)

```bash
# En la configuraciÃ³n de la API Key
1. Click en "Restrict Key"
2. En "API restrictions" seleccionar:
   - "Restrict key"
   - Marcar solo "Cloud Speech-to-Text API"
3. Guardar
```

**Â¿Por quÃ© restringir?**
- Si la key se filtra, solo puede usarse para Speech-to-Text
- No podrÃ­an usar la misma key para otras APIs de Google Cloud
- Principio de seguridad: mÃ­nimo privilegio necesario

### Variables de Entorno

```bash
# Archivo: .env.local
GOOGLE_CLOUD_API_KEY=AIza...
```

**Â¿Por quÃ© `.env.local` y no `.env`?**
- `.env.local` no se versiona en Git (estÃ¡ en `.gitignore`)
- Previene que las credenciales se suban accidentalmente
- `.env` se usa solo como plantilla documentada

---

## 4. Flujo Completo: De Voz a Texto

### Diagrama de Secuencia

```
Usuario                  Navegador              Next.js API          Google Cloud
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚ Click MicrÃ³fono          â”‚                        â”‚                    â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚ getUserMedia()         â”‚                    â”‚
  â”‚                          â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                    â”‚
  â”‚                          â”‚ â”‚ Grabando.. â”‚         â”‚                    â”‚
  â”‚                          â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                    â”‚
  â”‚                          â”‚ Acumulando chunks      â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚ Click Detener            â”‚                        â”‚                    â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚ Combinar chunks        â”‚                    â”‚
  â”‚                          â”‚ Crear Blob             â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚ POST /api/speech-to-text                    â”‚
  â”‚                          â”‚ FormData(audio.webm)   â”‚                    â”‚
  â”‚                          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚ Blobâ†’ArrayBuffer   â”‚
  â”‚                          â”‚                        â”‚ ArrayBufferâ†’Buffer â”‚
  â”‚                          â”‚                        â”‚ Bufferâ†’Base64      â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚ POST /v1/speech:recognize
  â”‚                          â”‚                        â”‚ {config, audio}    â”‚
  â”‚                          â”‚                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚ Decodificar Opus
  â”‚                          â”‚                        â”‚                    â”‚ Analizar frecuencias
  â”‚                          â”‚                        â”‚                    â”‚ Modelo de IA (espaÃ±ol)
  â”‚                          â”‚                        â”‚                    â”‚ Generar transcripciÃ³n
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚ {results: [...]}   â”‚
  â”‚                          â”‚                        â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚ Combinar results   â”‚
  â”‚                          â”‚                        â”‚ Extraer transcript â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚ {transcript: "hola"}   â”‚                    â”‚
  â”‚                          â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
  â”‚                          â”‚ setTranscript()        â”‚                    â”‚
  â”‚                          â”‚ Actualizar input       â”‚                    â”‚
  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚                    â”‚
  â”‚ Ve el texto en pantalla  â”‚                        â”‚                    â”‚
  â”‚                          â”‚                        â”‚                    â”‚
```

### Tiempo Estimado por Paso

1. **GrabaciÃ³n**: Variable (usuario decide)
2. **CombinaciÃ³n de chunks**: ~10-50ms
3. **Upload a Next.js**: ~100-500ms (depende de conexiÃ³n)
4. **ConversiÃ³n a base64**: ~10-30ms
5. **Request a Google Cloud**: ~200-800ms (depende de regiÃ³n)
6. **Procesamiento en Google Cloud**: ~500ms-2s (depende de duraciÃ³n del audio)
7. **Response**: ~100-300ms
8. **ActualizaciÃ³n UI**: ~10-50ms

**Total aproximado**: 1-4 segundos despuÃ©s de detener grabaciÃ³n

---

## 5. Manejo de Errores

### Errores Comunes y Soluciones

#### Error: "API key not valid"

```json
{
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key."
  }
}
```

**Causas**:
1. La API key estÃ¡ mal copiada
2. La API key fue eliminada
3. La API key no tiene permisos para Speech-to-Text

**SoluciÃ³n**:
```bash
# Verificar en .env.local
GOOGLE_CLOUD_API_KEY=AIzaSyC...  # â† Sin espacios, sin comillas

# Verificar en Google Cloud Console que la key existe
# Verificar que tiene restricciÃ³n a Speech-to-Text API
```

#### Error: "PERMISSION_DENIED"

```json
{
  "error": {
    "code": 403,
    "message": "Cloud Speech-to-Text API has not been used..."
  }
}
```

**Causa**: La API no estÃ¡ habilitada en el proyecto

**SoluciÃ³n**:
```bash
# Ir a: https://console.cloud.google.com/apis/library
# Buscar: Cloud Speech-to-Text API
# Click: Enable
```

#### Error: "Invalid audio format"

```json
{
  "error": {
    "code": 400,
    "message": "Invalid recognition 'config': bad encoding.."
  }
}
```

**Causa**: El encoding en config no coincide con el audio real

**SoluciÃ³n**:
```typescript
// Verificar que MediaRecorder usa:
mimeType: "audio/webm;codecs=opus"

// Y que la config en el API envÃ­a:
encoding: "WEBM_OPUS"  // â† Deben coincidir
```

#### Error: "Sample rate mismatch"

```json
{
  "error": {
    "code": 400,
    "message": "Sample rate must match..."
  }
}
```

**SoluciÃ³n**:
```typescript
// La mayorÃ­a de navegadores graban a 48kHz
// Usar siempre:
sampleRateHertz: 48000

// Si tienes dudas, puedes omitir este campo
// Google Cloud lo detectarÃ¡ automÃ¡ticamente
```

---

## 6. Optimizaciones Implementadas

### 1. Uso de Refs en Lugar de State para Chunks

```typescript
// âŒ MAL - Causa re-renders en cada chunk
const [audioChunks, setAudioChunks] = useState<Blob[]>([]);
mediaRecorder.ondataavailable = (event) => {
  setAudioChunks(prev => [...prev, event.data]); // Re-render!
};

// âœ… BIEN - Sin re-renders
const audioChunksRef = useRef<Blob[]>([]);
mediaRecorder.ondataavailable = (event) => {
  audioChunksRef.current.push(event.data); // No re-render
};
```

**Por quÃ©**: Durante la grabaciÃ³n pueden generarse 10-50 chunks. Cada `setState` causarÃ­a un re-render innecesario.

### 2. Cleanup de Streams

```typescript
const stopRecording = async () => {
  // ... detener grabaciÃ³n
  
  // Liberar recursos del micrÃ³fono
  const tracks = mediaRecorderRef.current?.stream.getTracks();
  tracks?.forEach((track) => track.stop());
  
  audioChunksRef.current = []; // Limpiar memoria
};
```

**Por quÃ©**: 
- Si no se detienen los tracks, el micrÃ³fono permanece activo (indicador rojo en navegador)
- Los chunks acumulados ocupan memoria RAM
- Es importante limpiar despuÃ©s de cada grabaciÃ³n

### 3. Estados de Carga Diferenciados

```typescript
const [isRecording, setIsRecording] = useState(false);
const [isTranscribing, setIsTranscribing] = useState(false);
```

**Por quÃ© dos estados separados**:
- Permiten diferentes indicadores visuales
- `isRecording`: Muestra botÃ³n rojo pulsante
- `isTranscribing`: Muestra "Transcribing audio..." con spinner
- El usuario entiende en quÃ© fase estÃ¡ el proceso

---

## 7. Consideraciones de Costos

### Cuotas Gratuitas de Google Cloud

```
Primeros 60 minutos/mes: GRATIS
DespuÃ©s de 60 minutos: $0.006 USD por cada 15 segundos

Ejemplo:
- 10 grabaciones de 30 segundos cada una = 5 minutos
- 5 minutos << 60 minutos â†’ GRATIS
- 200 grabaciones de 30 segundos = 100 minutos
- 40 minutos excedentes = 160 chunks de 15s
- 160 Ã— $0.006 = $0.96 USD
```

### Estrategias para Reducir Costos en ProducciÃ³n

#### 1. Limitar DuraciÃ³n de Grabaciones

```typescript
const MAX_RECORDING_TIME = 30000; // 30 segundos

const startRecording = async () => {
  // ... cÃ³digo existente
  
  const timeoutId = setTimeout(() => {
    stopRecording();
    alert("MÃ¡ximo 30 segundos de grabaciÃ³n");
  }, MAX_RECORDING_TIME);
  
  // Guardar timeout para limpiar si detienen manualmente
  timeoutRef.current = timeoutId;
};
```

#### 2. Comprimir Audio Antes de Enviar

```typescript
// Reducir bitrate de Opus
const mediaRecorder = new MediaRecorder(stream, {
  mimeType: "audio/webm;codecs=opus",
  audioBitsPerSecond: 16000, // Menor calidad = menor tamaÃ±o
});
```

**Nota**: Menor bitrate = menor precisiÃ³n de transcripciÃ³n. Encontrar el balance.

#### 3. CachÃ© de Transcripciones Comunes

```typescript
// En producciÃ³n, cachear frases comunes
const transcriptionCache = new Map<string, string>();

// Generar hash del audio
const audioHash = await crypto.subtle.digest('SHA-256', arrayBuffer);
const cached = transcriptionCache.get(audioHash);

if (cached) {
  return NextResponse.json({ transcript: cached });
}

// Si no estÃ¡ en cachÃ©, llamar a Google Cloud
// Guardar en cachÃ© el resultado
```

---

## 8. Testing y Debugging

### Logs Ãštiles

En el endpoint Next.js:

```typescript
export async function POST(request: NextRequest) {
  console.log("ğŸ¤ Speech-to-text request received");
  
  const audioFile = formData.get("audio") as Blob;
  console.log("ğŸ“Š Audio size:", audioFile.size, "bytes");
  console.log("ğŸ“Š Audio type:", audioFile.type);
  
  const base64Audio = buffer.toString("base64");
  console.log("ğŸ“Š Base64 length:", base64Audio.length);
  
  const response = await fetch(...);
  console.log("â˜ï¸  Google Cloud response status:", response.status);
  
  const data = await response.json();
  console.log("ğŸ“ Transcription:", data);
}
```

### Verificar Audio en el Navegador

```typescript
// En use-speech-to-text.ts
const audioBlob = new Blob(audioChunksRef.current, {
  type: "audio/webm;codecs=opus",
});

// Debug: Reproducir el audio grabado
const audioUrl = URL.createObjectURL(audioBlob);
const audio = new Audio(audioUrl);
audio.play(); // Escuchar lo que se grabÃ³

console.log("Audio blob size:", audioBlob.size);
console.log("Audio blob type:", audioBlob.type);
```

### Herramientas de Debugging

1. **Chrome DevTools Network Tab**:
   - Ver el request a `/api/speech-to-text`
   - Verificar que el FormData contiene el audio
   - Ver tiempo de respuesta

2. **Google Cloud Console**:
   - Ir a "APIs & Services" > "Dashboard"
   - Ver requests por minuto
   - Ver errores en tiempo real

3. **Logs de Next.js**:
   ```bash
   # Ver logs en consola durante desarrollo
   pnpm dev
   
   # Filtrar solo logs de speech-to-text
   pnpm dev | grep "speech"
   ```

---

## 9. Alternativas Consideradas

### Â¿Por quÃ© Google Cloud y no otras opciones?

#### Whisper de OpenAI
```
Pros:
- Alta precisiÃ³n
- Soporte de 99+ idiomas
- Modelo de cÃ³digo abierto

Contras:
- Requiere GPU para ejecutar localmente
- API mÃ¡s costosa que Google Cloud
- Mayor latencia
```

#### Web Speech API del Navegador
```javascript
const recognition = new webkitSpeechRecognition();
recognition.start();
```

```
Pros:
- Completamente gratis
- Sin latencia de red
- Funciona offline

Contras:
- Solo funciona en Chrome/Edge
- Requiere conexiÃ³n a internet (usa servicios de Google)
- No permite configuraciÃ³n avanzada
- DifÃ­cil de debuggear
```

#### Azure Speech Service
```
Pros:
- Excelente precisiÃ³n
- Buena documentaciÃ³n
- IntegraciÃ³n con otros servicios Azure

Contras:
- MÃ¡s costoso que Google Cloud
- Requiere cuenta de Azure
- Curva de aprendizaje mÃ¡s alta
```

**ConclusiÃ³n**: Google Cloud ofrece el mejor balance de costo, precisiÃ³n y facilidad de implementaciÃ³n para una POC.

---

## 10. Mejoras Futuras

### 1. Streaming de Audio

Actualmente: Audio completo â†’ TranscripciÃ³n completa

Posible: Audio en tiempo real â†’ TranscripciÃ³n en tiempo real

```typescript
// Usar streaming recognition de Google Cloud
const stream = recognizeStream({
  config: {
    encoding: 'WEBM_OPUS',
    sampleRateHertz: 48000,
    languageCode: 'es-ES',
    interimResults: true, // Resultados parciales
  },
});

mediaRecorder.ondataavailable = (event) => {
  stream.write(event.data); // Enviar chunks en tiempo real
};

stream.on('data', (data) => {
  if (data.results[0].isFinal) {
    console.log(data.results[0].alternatives[0].transcript);
  }
});
```

**Ventajas**:
- Usuario ve transcripciÃ³n mientras habla
- Puede corregir sobre la marcha
- Mejor UX para grabaciones largas

### 2. DetecciÃ³n AutomÃ¡tica de Idioma

```typescript
// Remover languageCode fijo
config: {
  encoding: "WEBM_OPUS",
  sampleRateHertz: 48000,
  // languageCode: "es-ES", â† Comentar esto
  alternativeLanguageCodes: ["es-ES", "en-US", "fr-FR"], 
  enableAutomaticPunctuation: true, // Bonus: PuntuaciÃ³n automÃ¡tica
}
```

### 3. ConfiguraciÃ³n por Usuario

```typescript
// Permitir al usuario elegir el idioma
interface SpeechConfig {
  language: 'es-ES' | 'en-US' | 'pt-BR';
  sampleRate: 16000 | 48000;
}

const userConfig = getUserPreferences(); // Desde localStorage

config: {
  languageCode: userConfig.language,
  sampleRateHertz: userConfig.sampleRate,
}
```

---

## ConclusiÃ³n

Esta implementaciÃ³n de Google Cloud Speech-to-Text demuestra:

1. **Arquitectura Clara**: SeparaciÃ³n entre cliente (captura), servidor (proxy) y servicio (transcripciÃ³n)
2. **Seguridad**: API keys en servidor, nunca expuestas al cliente
3. **OptimizaciÃ³n**: Uso eficiente de refs, limpieza de recursos, estados diferenciados
4. **Manejo de Errores**: Validaciones en cada paso, mensajes claros
5. **Escalabilidad**: FÃ¡cil agregar features como streaming o mÃºltiples idiomas

El cÃ³digo estÃ¡ diseÃ±ado para ser:
- **Educativo**: Comentado y explicado en detalle
- **Mantenible**: Funciones pequeÃ±as con responsabilidades claras
- **Extensible**: FÃ¡cil agregar nuevas funcionalidades

---

## Referencias TÃ©cnicas

- [Google Cloud Speech-to-Text Documentation](https://cloud.google.com/speech-to-text/docs)
- [MediaRecorder API - MDN](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder)
- [Next.js API Routes](https://nextjs.org/docs/api-routes/introduction)
- [Opus Codec](https://opus-codec.org/)

---

**Universidad del Valle** | Proyecto Integrador II-01 | Octubre 2025

## DescripciÃ³n General

Este proyecto constituye una prueba de concepto (POC) desarrollada para el curso Proyecto Integrador II-01 de la Universidad del Valle. La aplicaciÃ³n integra tecnologÃ­as de inteligencia artificial conversacional con capacidades de reconocimiento de voz, permitiendo a los usuarios interactuar mediante texto o voz con un asistente basado en modelos de lenguaje de gran escala.

La arquitectura implementada combina Next.js como framework principal, Convex como base de datos reactiva en tiempo real, Google Cloud Speech-to-Text para la transcripciÃ³n de audio, y la API de OpenAI para la generaciÃ³n de respuestas conversacionales.

## Contexto AcadÃ©mico

**InstituciÃ³n**: Universidad del Valle  
**Curso**: Proyecto Integrador II-01  
**Equipo**: Golangers  
**PerÃ­odo**: Octubre 2025  
**Repository**: univalle-software-development/poc-audio

## Arquitectura del Sistema

### Stack TecnolÃ³gico

El proyecto se fundamenta en las siguientes tecnologÃ­as y servicios:

#### Frontend
- **Next.js 13.5.1**: Framework de React que proporciona renderizado del lado del servidor (SSR), generaciÃ³n de sitios estÃ¡ticos (SSG) y rutas de API serverless.
- **React 18.2.0**: Biblioteca principal para la construcciÃ³n de interfaces de usuario reactivas.
- **TypeScript 5.2.2**: Superset de JavaScript que aÃ±ade tipado estÃ¡tico, mejorando la robustez del cÃ³digo y la experiencia de desarrollo.
- **Tailwind CSS 3.3.3**: Framework de CSS utilitario que permite un diseÃ±o responsive y consistente mediante clases predefinidas.

#### Backend y Base de Datos
- **Convex 1.22.0**: Plataforma de backend que proporciona una base de datos reactiva con sincronizaciÃ³n en tiempo real, funciones serverless y gestiÃ³n de estado global.
- **OpenAI API**: Servicio de inteligencia artificial que provee acceso a modelos de lenguaje para la generaciÃ³n de respuestas conversacionales. El modelo implementado es GPT-5-nano, seleccionado por su balance entre rendimiento y costo para entornos de prueba.

#### Servicios de Terceros
- **Google Cloud Speech-to-Text API**: Servicio de reconocimiento automÃ¡tico de voz (ASR) que convierte audio hablado en texto transcrito. Configurado para soportar espaÃ±ol (es-ES) como idioma principal e inglÃ©s (en-US) como alternativa.

#### Bibliotecas Complementarias
- **Radix UI**: Conjunto de componentes de interfaz accesibles y sin estilo predefinido, utilizados como base para elementos como diÃ¡logos, menÃºs desplegables y tooltips.
- **React Textarea Autosize**: Componente que ajusta automÃ¡ticamente la altura del Ã¡rea de texto segÃºn el contenido.
- **Lucide React**: Biblioteca de iconos SVG optimizados para React.

### PatrÃ³n de Arquitectura

La aplicaciÃ³n sigue una arquitectura de tres capas claramente definidas:

#### Capa de PresentaciÃ³n
Compuesta por componentes React organizados en el directorio `components/`. Los componentes principales incluyen:

- **Chat Component**: Orquesta la interfaz de usuario del chat, gestionando la visualizaciÃ³n de mensajes, el input del usuario y los controles de interacciÃ³n.
- **ChatMessage Component**: Renderiza mensajes individuales con formato diferenciado para mensajes del usuario y del asistente.
- **Loading Indicators**: Componentes especializados que proporcionan retroalimentaciÃ³n visual durante operaciones asÃ­ncronas como la transcripciÃ³n de audio y la generaciÃ³n de respuestas.

#### Capa de LÃ³gica de Negocio
Implementada mediante hooks personalizados de React y acciones de Convex:

- **useSpeechToText**: Hook que encapsula toda la lÃ³gica relacionada con la grabaciÃ³n de audio mediante MediaRecorder API, la transmisiÃ³n de datos al servidor y la gestiÃ³n de estados de transcripciÃ³n.
- **useConvexChat**: Hook que gestiona el estado del chat, incluyendo mensajes, entrada de usuario y comunicaciÃ³n con el backend de Convex.
- **Convex Actions**: Funciones serverless que ejecutan operaciones asÃ­ncronas como llamadas a APIs externas (OpenAI, Google Cloud).

#### Capa de Datos
Gestionada por Convex, que proporciona:

- **Schema Definitions**: Definiciones tipadas de las estructuras de datos almacenadas.
- **Queries**: Funciones reactivas que recuperan datos y se actualizan automÃ¡ticamente cuando los datos cambian.
- **Mutations**: Funciones que modifican el estado de la base de datos de forma transaccional.

## Flujo de Datos y ComunicaciÃ³n

### Flujo de InteracciÃ³n por Texto

1. El usuario ingresa texto en el componente de input.
2. Al enviar, se dispara una mutaciÃ³n de Convex que almacena el mensaje del usuario en la base de datos.
3. La mutaciÃ³n desencadena una acciÃ³n que envÃ­a el historial de conversaciÃ³n al modelo de OpenAI.
4. OpenAI procesa el contexto y genera una respuesta.
5. La respuesta se almacena en Convex mediante otra mutaciÃ³n.
6. Los queries reactivos detectan el cambio y actualizan automÃ¡ticamente la interfaz, mostrando la respuesta del asistente.

### Flujo de InteracciÃ³n por Voz

1. El usuario activa el botÃ³n de micrÃ³fono, iniciando la captura de audio mediante la MediaRecorder API del navegador.
2. Durante la grabaciÃ³n, el audio se acumula en fragmentos (chunks) almacenados en memoria.
3. Al detener la grabaciÃ³n, los fragmentos se combinan en un Blob con formato WebM Opus.
4. El hook `useSpeechToText` transmite el Blob mediante FormData a la ruta API `/api/speech-to-text`.
5. El endpoint de Next.js recibe el audio, lo convierte a formato base64 y lo envÃ­a a Google Cloud Speech-to-Text API.
6. Google Cloud procesa el audio aplicando algoritmos de reconocimiento de voz y retorna la transcripciÃ³n en formato JSON.
7. El texto transcrito se recibe en el hook y se actualiza en el estado local.
8. Un efecto de React detecta la actualizaciÃ³n y coloca automÃ¡ticamente la transcripciÃ³n en el input del chat.
9. El usuario puede revisar, editar y enviar el texto transcrito, siguiendo entonces el flujo de interacciÃ³n por texto.

## ImplementaciÃ³n de Speech-to-Text

### ConfiguraciÃ³n de Google Cloud Platform

La integraciÃ³n con Google Cloud Speech-to-Text requiere una configuraciÃ³n previa en la plataforma:

1. **CreaciÃ³n del Proyecto**: Se establece un proyecto dedicado en Google Cloud Platform, lo cual permite aislar recursos, configuraciones y facturaciÃ³n.

2. **HabilitaciÃ³n de la API**: La Cloud Speech-to-Text API debe habilitarse explÃ­citamente para el proyecto. Las APIs de Google Cloud estÃ¡n deshabilitadas por defecto como medida de seguridad y control de costos.

3. **GeneraciÃ³n de Credenciales**: Se crea una API Key que actÃºa como mecanismo de autenticaciÃ³n. Esta clave permite que las solicitudes desde la aplicaciÃ³n sean autorizadas por Google Cloud.

4. **RestricciÃ³n de Credenciales**: Como prÃ¡ctica de seguridad, la API Key se restringe para que Ãºnicamente pueda utilizarse con la Speech-to-Text API, minimizando el riesgo en caso de exposiciÃ³n accidental.

### Arquitectura del Endpoint

El endpoint `/api/speech-to-text` implementado como Next.js API Route proporciona una capa de abstracciÃ³n entre el cliente y Google Cloud:

**Razones para esta arquitectura**:
- **Seguridad**: La API Key de Google Cloud permanece en el servidor, nunca se expone al navegador del cliente.
- **Control**: Permite implementar validaciones, rate limiting y manejo centralizado de errores.
- **AbstracciÃ³n**: Desacopla el cliente de los detalles de implementaciÃ³n de Google Cloud, facilitando futuros cambios de proveedor.

**Proceso de transcripciÃ³n**:
1. RecepciÃ³n del audio en formato Blob mediante FormData.
2. ConversiÃ³n del Blob a ArrayBuffer y posterior codificaciÃ³n en base64.
3. ConstrucciÃ³n de la solicitud HTTP a Google Cloud con la configuraciÃ³n apropiada.
4. Procesamiento de la respuesta y extracciÃ³n del texto transcrito.
5. Retorno de la transcripciÃ³n al cliente en formato JSON.

### ConfiguraciÃ³n de Audio

Los parÃ¡metros de configuraciÃ³n para el reconocimiento de voz fueron seleccionados considerando:

- **Encoding (WEBM_OPUS)**: Formato estÃ¡ndar utilizado por MediaRecorder en navegadores modernos. Opus proporciona buena calidad de audio con tasas de compresiÃ³n eficientes.

- **Sample Rate (48000 Hz)**: Tasa de muestreo de alta fidelidad que captura un amplio rango de frecuencias vocales, mejorando la precisiÃ³n del reconocimiento.

- **Language Code (es-ES)**: Configurado para espaÃ±ol de EspaÃ±a como idioma principal, con posibilidad de ajuste segÃºn la regiÃ³n objetivo.

- **Alternative Language Codes (en-US)**: InglÃ©s americano como idioma de respaldo, permitiendo que el sistema funcione si detecta entrada en inglÃ©s.

## IntegraciÃ³n con OpenAI

### SelecciÃ³n del Modelo

El proyecto utiliza GPT-5-nano como modelo de lenguaje, una decisiÃ³n fundamentada en:

- **EconomÃ­a**: Con lÃ­mites de 200,000 tokens por minuto y 2,000,000 tokens por dÃ­a, resulta apropiado para un entorno de prueba de concepto sin incurrir en costos significativos.

- **Rendimiento**: Proporciona respuestas de calidad suficiente para validar la integraciÃ³n y el flujo de usuario.

- **Disponibilidad**: Los lÃ­mites de 500 requests por minuto permiten pruebas simultÃ¡neas con mÃºltiples usuarios.

### ConfiguraciÃ³n del System Prompt

El system prompt se configurÃ³ de manera genÃ©rica y sin restricciones temÃ¡ticas:

```
You are ChatGPT, a helpful assistant powered by OpenAI. 
You can assist with a wide range of topics and tasks.
```

Esta configuraciÃ³n permite que el asistente responda a cualquier consulta sin limitarse a dominios especÃ­ficos, facilitando la validaciÃ³n de la funcionalidad general del sistema.

### GestiÃ³n de Contexto

Convex Actions maneja la comunicaciÃ³n con OpenAI de la siguiente manera:

1. **RecuperaciÃ³n del Historial**: Se obtienen todos los mensajes de la conversaciÃ³n actual desde Convex.
2. **ConstrucciÃ³n del Contexto**: Los mensajes se formatean segÃºn la estructura esperada por la API de OpenAI, incluyendo roles (user, assistant, system).
3. **InclusiÃ³n del System Prompt**: Se antepone el prompt del sistema al inicio del array de mensajes.
4. **EnvÃ­o y Procesamiento**: La solicitud se envÃ­a a OpenAI y la respuesta se procesa.
5. **Persistencia**: La respuesta del modelo se almacena en Convex, manteniendo la continuidad conversacional.

## Componentes de Interfaz

### Indicadores de Carga

Se implementaron dos tipos de indicadores visuales para mejorar la experiencia del usuario:

#### Indicador de TranscripciÃ³n
Aparece como un badge flotante sobre el Ã¡rea de input durante el proceso de transcripciÃ³n. Consiste en tres puntos animados con rebote secuencial acompaÃ±ados del texto "Transcribing audio...". El diseÃ±o utiliza:
- Fondo gris claro (zinc-100) con bordes redondeados completos (pill shape).
- Sombra suave para elevaciÃ³n visual.
- Posicionamiento absoluto que no interfiere con otros elementos.

#### Indicador de Procesamiento de IA
Se muestra en el Ã¡rea de mensajes cuando el modelo estÃ¡ generando una respuesta. Presenta:
- Avatar circular con gradiente azul (blue-400 a blue-600) conteniendo las letras "AI".
- Puntos animados con rebote secuencial.
- DiseÃ±o consistente con los mensajes del chat para coherencia visual.

Ambos indicadores utilizan animaciones CSS nativas de Tailwind, evitando JavaScript adicional y manteniendo un rendimiento Ã³ptimo.

### Componente de Chat

El componente principal del chat implementa:

- **Layout Flexible**: Utiliza Flexbox CSS para distribuir el espacio entre la lista de mensajes y el Ã¡rea de input, adaptÃ¡ndose al viewport del dispositivo.

- **Scroll AutomÃ¡tico**: Implementa auto-scroll al final de la conversaciÃ³n cuando llegan nuevos mensajes, con lÃ³gica para evitar scroll en la carga inicial.

- **Responsive Design**: Adapta el diseÃ±o mediante breakpoints de Tailwind, ajustando padding, tamaÃ±os de fuente y distribuciÃ³n de elementos segÃºn el tamaÃ±o de pantalla.

- **Manejo de Estados**: Coordina mÃºltiples estados (isLoading, isRecording, isTranscribing) para actualizar la interfaz apropiadamente.

## GestiÃ³n de Variables de Entorno

### Estrategia de ConfiguraciÃ³n

El proyecto distingue entre variables de entorno pÃºblicas y privadas:

**Variables PÃºblicas** (prefijo `NEXT_PUBLIC_`):
- `NEXT_PUBLIC_CONVEX_URL`: URL del deployment de Convex, accesible desde el navegador para establecer la conexiÃ³n WebSocket.

**Variables Privadas** (sin prefijo):
- `GOOGLE_CLOUD_API_KEY`: Credencial de Google Cloud, disponible Ãºnicamente en el servidor.
- `OPENAI_API_KEY`: Credencial de OpenAI, almacenada en el dashboard de Convex para uso en Actions.

### Archivos de ConfiguraciÃ³n

- **`.env.local`**: Archivo local no versionado que contiene credenciales sensibles para desarrollo.
- **`.env`**: Archivo versionado que documenta las variables requeridas sin incluir valores reales, sirviendo como plantilla.
- **`.gitignore`**: Configurado para excluir `.env.local` y otros archivos sensibles del control de versiones.

## Seguridad

### PrÃ¡cticas Implementadas

1. **SeparaciÃ³n de Credenciales**: Las API keys nunca se exponen al cliente, manteniÃ©ndose exclusivamente en el servidor.

2. **RestricciÃ³n de API Keys**: La clave de Google Cloud estÃ¡ limitada a una sola API, reduciendo el impacto de una posible filtraciÃ³n.

3. **Variables de Entorno**: Uso de variables de entorno en lugar de hardcodear credenciales en el cÃ³digo fuente.

4. **ValidaciÃ³n en Endpoints**: Los endpoints de API verifican la existencia de datos requeridos antes de procesarlos.

5. **Manejo de Errores**: ImplementaciÃ³n de bloques try-catch con logs detallados para facilitar debugging sin exponer informaciÃ³n sensible al cliente.

### Consideraciones para ProducciÃ³n

Para un despliegue en producciÃ³n, se recomiendan mejoras adicionales:

- **AutenticaciÃ³n**: Implementar verificaciÃ³n de identidad del usuario antes de permitir acceso a funcionalidades.
- **Rate Limiting**: Establecer lÃ­mites de requests por usuario para prevenir abuso.
- **CORS**: Configurar polÃ­ticas de Cross-Origin Resource Sharing apropiadas.
- **SanitizaciÃ³n de Input**: Validar y limpiar entradas de usuario para prevenir inyecciones.
- **Monitoreo**: Implementar logging y alertas para detectar comportamientos anÃ³malos.

## Estructura del Proyecto

```
poc-audio/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ speech-to-text/
â”‚   â”‚       â””â”€â”€ route.ts          # Endpoint de transcripciÃ³n
â”‚   â”œâ”€â”€ globals.css               # Estilos globales y Tailwind
â”‚   â”œâ”€â”€ layout.tsx                # Layout principal de la aplicaciÃ³n
â”‚   â”œâ”€â”€ page.tsx                  # PÃ¡gina principal del chat
â”‚   â””â”€â”€ providers.tsx             # Provider de Convex
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat.tsx                  # Componente principal del chat
â”‚   â”œâ”€â”€ chat-message.tsx          # Renderizado de mensajes individuales
â”‚   â”œâ”€â”€ convex-chat-provider.tsx  # Provider y lÃ³gica del chat
â”‚   â”œâ”€â”€ loading-dots.tsx          # Indicadores de carga
â”‚   â”œâ”€â”€ navbar.tsx                # Barra de navegaciÃ³n
â”‚   â””â”€â”€ footer.tsx                # Pie de pÃ¡gina
â”œâ”€â”€ convex/
â”‚   â”œâ”€â”€ chat.ts                   # Queries y mutations del chat
â”‚   â”œâ”€â”€ multiModelAI.ts           # Actions para OpenAI
â”‚   â”œâ”€â”€ speechToText.ts           # ConfiguraciÃ³n de Speech-to-Text
â”‚   â””â”€â”€ schema.ts                 # DefiniciÃ³n del schema de datos
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ use-speech-to-text.ts     # Hook de grabaciÃ³n y transcripciÃ³n
â”‚   â””â”€â”€ use-toast.ts              # Sistema de notificaciones
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ utils.ts                  # Utilidades compartidas
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ google-cloud-speech-to-text-setup.md  # GuÃ­a de configuraciÃ³n
â”‚   â”œâ”€â”€ loading-indicators.md                  # DocumentaciÃ³n de indicadores
â”‚   â””â”€â”€ environment-variables.md               # Variables de entorno
â””â”€â”€ public/
    â””â”€â”€ golangers.webp            # Logo del equipo
```

## Desarrollo y EjecuciÃ³n

### Requisitos del Sistema

- Node.js versiÃ³n 18 o superior
- pnpm como gestor de paquetes
- Cuenta de Google Cloud Platform con facturaciÃ³n habilitada
- Cuenta de OpenAI con crÃ©ditos disponibles
- Cuenta de Convex (gratuita para desarrollo)

### ConfiguraciÃ³n Inicial

1. **ClonaciÃ³n del Repositorio**:
   ```bash
   git clone https://github.com/univalle-software-development/poc-audio.git
   cd poc-audio
   ```

2. **InstalaciÃ³n de Dependencias**:
   ```bash
   pnpm install
   ```

3. **ConfiguraciÃ³n de Convex**:
   ```bash
   npx convex dev
   ```
   Este comando inicializa un proyecto de Convex, genera el cÃ³digo cliente y actualiza el archivo `.env` con la URL del deployment.

4. **ConfiguraciÃ³n de Variables de Entorno**:
   Crear archivo `.env.local` con:
   ```
   GOOGLE_CLOUD_API_KEY=<tu-api-key-de-google-cloud>
   ```

5. **ConfiguraciÃ³n de OpenAI en Convex**:
   - Acceder al dashboard de Convex
   - Navegar a Settings â†’ Environment Variables
   - Agregar `OPENAI_API_KEY` con el valor correspondiente

### EjecuciÃ³n en Desarrollo

Para iniciar el servidor de desarrollo:

```bash
pnpm dev
```

La aplicaciÃ³n estarÃ¡ disponible en `http://localhost:3000`.

En una terminal separada, mantener Convex en modo desarrollo:

```bash
npx convex dev
```

Esto habilita hot-reload tanto para el frontend como para las funciones de Convex.

### CompilaciÃ³n para ProducciÃ³n

```bash
pnpm build
pnpm start
```

El comando `build` genera una versiÃ³n optimizada de la aplicaciÃ³n, mientras que `start` sirve esta versiÃ³n en modo producciÃ³n.

## Consideraciones de Costos

### Google Cloud Speech-to-Text

- **Capa Gratuita**: 60 minutos de transcripciÃ³n mensual sin costo.
- **Costo Adicional**: $0.006 USD por cada 15 segundos de audio procesado despuÃ©s de agotar la capa gratuita.
- **Estrategia de OptimizaciÃ³n**: Para una POC, la capa gratuita es suficiente. En producciÃ³n, considerar lÃ­mites de duraciÃ³n por grabaciÃ³n y caching de transcripciones comunes.

### OpenAI API

- **Modelo GPT-5-nano**: DiseÃ±ado para ser econÃ³mico con lÃ­mites de 200,000 TPM (tokens por minuto).
- **Monitoreo**: Revisar periÃ³dicamente el uso en el dashboard de OpenAI para evitar costos inesperados.
- **Alternativas**: Considerar modelos mÃ¡s pequeÃ±os o proveedores alternativos para reducir costos en producciÃ³n.

### Convex

- **Plan Gratuito**: Suficiente para desarrollo y proyectos pequeÃ±os.
- **Escalabilidad**: Los planes pagos se ajustan segÃºn uso, facilitando la transiciÃ³n a producciÃ³n.

## Mejoras Futuras

### Funcionalidades TÃ©cnicas

1. **DetecciÃ³n AutomÃ¡tica de Idioma**: Implementar detecciÃ³n del idioma hablado antes de enviar a Google Cloud, mejorando la precisiÃ³n en entornos multilingÃ¼es.

2. **VisualizaciÃ³n de Forma de Onda**: Agregar representaciÃ³n visual de la amplitud del audio durante la grabaciÃ³n para mejor feedback al usuario.

3. **EdiciÃ³n de TranscripciÃ³n**: Interfaz de confirmaciÃ³n que permita al usuario revisar y corregir la transcripciÃ³n antes de enviarla como mensaje.

4. **Historial de Conversaciones**: Implementar persistencia de sesiones mÃºltiples con capacidad de recuperar conversaciones anteriores.

5. **Soporte de Voz Continua**: Permitir entrada de voz sin necesidad de detener manualmente, usando detecciÃ³n de silencios.

### Optimizaciones de Rendimiento

1. **Lazy Loading**: Cargar componentes pesados bajo demanda para reducir el tamaÃ±o del bundle inicial.

2. **MemoizaciÃ³n**: Aplicar React.memo y useMemo en componentes que renderizan frecuentemente.

3. **CompresiÃ³n de Audio**: Implementar compresiÃ³n adicional del audio antes de enviarlo a Google Cloud para reducir latencia y costos.

4. **Caching**: Implementar estrategias de cache para respuestas comunes del modelo de IA.

### Mejoras de UX

1. **RetroalimentaciÃ³n HÃ¡ptica**: En dispositivos mÃ³viles, proporcionar vibraciÃ³n al iniciar/detener grabaciÃ³n.

2. **Atajos de Teclado**: Implementar shortcuts para acciones comunes (Ctrl+Enter para enviar, etc.).

3. **Temas**: Sistema de temas claro/oscuro con preferencia persistente.

4. **Accesibilidad**: Mejorar navegaciÃ³n por teclado, etiquetas ARIA y soporte para lectores de pantalla.

## Lecciones Aprendidas

### IntegraciÃ³n de APIs Externas

La experiencia de integrar mÃºltiples APIs externas (Google Cloud, OpenAI) demostrÃ³ la importancia de una arquitectura bien diseÃ±ada con capas de abstracciÃ³n claras. El uso de Next.js API Routes como capa intermedia resultÃ³ valioso tanto para seguridad como para mantenibilidad.

### Manejo de Estado en Aplicaciones Reactivas

Convex como backend reactivo simplificÃ³ significativamente la sincronizaciÃ³n de estado entre cliente y servidor. La actualizaciÃ³n automÃ¡tica de la UI cuando cambian los datos elimina cÃ³digo boilerplate y reduce errores.

### Experiencia de Usuario en Aplicaciones de IA

Los indicadores de carga resultaron cruciales para la percepciÃ³n de velocidad y confiabilidad de la aplicaciÃ³n. Incluso con tiempos de respuesta de 2-3 segundos, la retroalimentaciÃ³n visual apropiada mantiene al usuario informado y reduce la frustraciÃ³n.

### GestiÃ³n de Credenciales

La separaciÃ³n estricta entre variables pÃºblicas y privadas, junto con el uso de servicios gestionados (Convex) para variables sensibles, proporciona seguridad sin comprometer la experiencia de desarrollo.

## Conclusiones

Este proyecto de prueba de concepto demuestra exitosamente la viabilidad tÃ©cnica de integrar tecnologÃ­as de reconocimiento de voz con sistemas de inteligencia artificial conversacional. La arquitectura implementada es escalable, mantenible y sigue las mejores prÃ¡cticas de desarrollo web moderno.

Las tecnologÃ­as seleccionadas (Next.js, Convex, Google Cloud, OpenAI) se complementan efectivamente, cada una aportando capacidades especializadas que juntas forman un sistema cohesivo. La separaciÃ³n de responsabilidades entre frontend, backend y servicios externos facilita futuras extensiones y modificaciones.

El proyecto sirve como base sÃ³lida para futuras iteraciones que podrÃ­an incluir caracterÃ­sticas mÃ¡s avanzadas como anÃ¡lisis de sentimientos, traducciÃ³n en tiempo real, o integraciÃ³n con sistemas de telefonÃ­a para asistentes virtuales.

## Referencias

### DocumentaciÃ³n Oficial

- Next.js: https://nextjs.org/docs
- React: https://react.dev
- Convex: https://docs.convex.dev
- Google Cloud Speech-to-Text: https://cloud.google.com/speech-to-text/docs
- OpenAI API: https://platform.openai.com/docs
- Tailwind CSS: https://tailwindcss.com/docs

### Recursos TÃ©cnicos

- MediaRecorder API: https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder
- Web Audio API: https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API
- TypeScript Handbook: https://www.typescriptlang.org/docs/handbook/intro.html
- React Hooks: https://react.dev/reference/react

### ArtÃ­culos y GuÃ­as

- Best Practices for API Keys: https://cloud.google.com/docs/authentication/api-keys
- Serverless Architecture: https://martinfowler.com/articles/serverless.html
- React Performance Optimization: https://react.dev/learn/render-and-commit

## Contacto y ContribuciÃ³n

Este proyecto fue desarrollado como parte del curso Proyecto Integrador II-01 de la Universidad del Valle por el equipo Golangers.

Para consultas acadÃ©micas o tÃ©cnicas relacionadas con el proyecto, contactar a travÃ©s del repositorio oficial en GitHub: https://github.com/univalle-software-development/poc-audio

---

**Fecha de Ãšltima ActualizaciÃ³n**: Octubre 7, 2025  
**VersiÃ³n**: 1.0.0  
**Licencia**: MIT
